# ğŸ§¾ DevOps Dashboard Cheat Sheet

**How to read dashboards & diagnose issues like a pro**

---

## ğŸ§© The Big Picture â€” Why Dashboards Matter

Dashboards are not just â€œpretty graphsâ€ â€” they answer **3 core questions**:

1. **Is it up?** â†’ availability (can users access it?)
2. **Is it fast?** â†’ performance (is it responsive?)
3. **Is it healthy?** â†’ resource health (can it stay running?)

ğŸ‘‰ Always think in terms of **symptoms â†’ cause**.
For example: *â€œWebsite is slowâ€ (symptom) â†’ â€œdatabase CPU 100%â€ (cause).*

---

## ğŸŸ¢ Core Areas to Monitor

### 1. **CPU**

* **What to look for:**

  * `% usage` per core and overall.
  * Breakdown: **system**, **user**, **iowait**.
* **Rules of thumb:**

  * <70% sustained â†’ healthy.
  * > 90% sustained â†’ CPU bottleneck.
* **What it means:**

  * **High user CPU** â†’ app code is working hard (expected under load).
  * **High system CPU** â†’ kernel, syscalls, maybe networking overhead.
  * **High iowait** â†’ CPU is waiting on disk/network â†’ possible I/O bottleneck.
* **Actions:**

  * Add replicas / scale service.
  * Profile app (optimize queries, caching).
  * Investigate I/O subsystem if iowait high.

---

### 2. **Memory (RAM)**

* **What to look for:**

  * Total used vs total available.
  * Cache/buffers vs actual application usage.
  * Swap usage.
* **Rules of thumb:**

  * If memory is â€œfullâ€ but mostly cache â†’ not a problem (Linux uses free RAM for caching).
  * If swap is active â†’ real memory pressure.
* **What it means:**

  * **High app usage** â†’ possible memory leak or undersized instance.
  * **High swap usage** â†’ system thrashing, huge slowdown.
* **Actions:**

  * Restart leaking service.
  * Add more RAM.
  * Optimize memory-heavy queries.

---

### 3. **Disk / Storage**

* **Metrics:**

  * Disk usage %.
  * IOPS (reads/writes per second).
  * Latency (avg read/write ms).
* **Rules of thumb:**

  * > 80% disk full â†’ plan cleanup/expansion.
  * Latency >20ms on SSDs â†’ bottleneck.
* **What it means:**

  * High latency + high iowait â†’ storage bottleneck.
  * High disk usage â†’ risk of system crash when full.
* **Actions:**

  * Clear logs / rotate.
  * Scale to larger disks.
  * Use faster storage (NVMe, SSD).

---

### 4. **Network**

* **Metrics:**

  * Bandwidth in/out.
  * Packet drops/errors.
  * Latency / RTT.
* **Rules of thumb:**

  * Bandwidth near link limit (e.g. 1Gbps NIC at 950Mbps) â†’ saturation.
  * Packet drops/errors >0 â†’ network health issue.
* **What it means:**

  * High outbound traffic â†’ app serving lots of data (normal or DDoS).
  * Latency spikes â†’ congestion, routing problems.
* **Actions:**

  * Scale horizontally (more nodes).
  * Throttle heavy clients.
  * Investigate load balancer.

---

### 5. **Containers / Pods**

* **Metrics:**

  * CPU & memory per container.
  * Container restarts (counter).
  * Pod status (running, crashloop).
* **Red flags:**

  * Containers restarting repeatedly â†’ crashloop, misconfiguration.
  * CPU/memory throttling in Kubernetes.
* **What it means:**

  * Misconfigured resource limits.
  * App bugs (OOMKilled, segfaults).
* **Actions:**

  * Check logs for container crash cause.
  * Adjust requests/limits in K8s.
  * Add replicas for load.

---

### 6. **Application Level**

* **Key metrics (the â€œGolden Signalsâ€ from Google SRE):**

  * **Latency** â†’ how long requests take.
  * **Traffic** â†’ requests per second.
  * **Errors** â†’ % of failed requests.
  * **Saturation** â†’ how â€œfullâ€ the system is (queues, memory).
* **Interpretation:**

  * High latency + high errors â†’ app/service bottleneck.
  * High latency + low CPU/memory â†’ external dependency issue.
  * High traffic spikes â†’ expected? or DDoS?
* **Actions:**

  * Scale service horizontally.
  * Add caching layer.
  * Optimize slow queries.

---

### 7. **Databases**

* **Metrics:**

  * Query throughput (QPS).
  * Query latency.
  * Locks, deadlocks.
  * Buffer cache hit ratio.
* **Red flags:**

  * Slow queries â†’ long latency spikes.
  * Lock waits â†’ contention.
* **Actions:**

  * Add indexes.
  * Optimize queries.
  * Add read replicas.

---

### 8. **Logs & Events**

* Dashboards often link to logs (via Loki, ELK, etc.).
* Use them to confirm the *why* behind metrics.

---

## ğŸš¨ Diagnosis by Symptom

| Symptom                  | Likely Cause                                      | Where to Look                   |
| ------------------------ | ------------------------------------------------- | ------------------------------- |
| High latency (slow site) | CPU/memory saturated, DB slow, network congestion | CPU, memory, DB dashboards      |
| Frequent 500 errors      | App crash, DB errors, bad config                  | App logs, DB metrics            |
| Nodes going down         | Out of memory, disk full, network partition       | Node exporter, disk usage       |
| Container restarts       | Misconfig, OOMKilled, bad healthcheck             | Container/pod dashboards        |
| Traffic spike            | Legit user load vs DDoS                           | Network + load balancer metrics |
| Disk full alerts         | Logs, data growth, temp files                     | Disk usage dashboard            |

---

## ğŸ› ï¸ Method: How to Read a Dashboard Like a Pro

1. **Start broad â†’ drill down**

   * Begin with system overview (CPU/mem/disk).
   * Narrow to container/pod â†’ app â†’ DB.
2. **Look for correlations**

   * High CPU at same time as latency spikes?
   * High iowait + disk latency â†’ storage problem.
3. **Timeline matters**

   * Spikes vs sustained trends tell different stories.
4. **Always check for â€œinnocent victimsâ€**

   * If all pods restart at once â†’ node issue, not app bug.

---

## ğŸ¯ World-Class Habits

* Always correlate **metrics + logs**.
* Watch **rate of change**, not just absolute numbers (e.g., 10GB logs written/hour).
* Build mental models: *Traffic â†‘ â†’ CPU â†‘ â†’ Latency â†‘* is expected. If not, dig deeper.
* Treat dashboards as **hypothesis tools**, not truth â€” confirm with logs, traces, configs.

---

## ğŸ”‘ TL;DR Cheatsheet

* **CPU >90% sustained** â†’ bottleneck.
* **Memory + swap high** â†’ thrashing.
* **Disk >80% full** â†’ expand.
* **Disk latency >20ms (SSD)** â†’ bottleneck.
* **Network drops/errors** â†’ faulty NIC or congestion.
* **Container restarts** â†’ crashloop (logs!).
* **Latency + errors** â†’ app/db issue.
* **Traffic spike** â†’ scale or DDoS check.

---

## ğŸ§­ Troubleshooting Flow â€” From Alert â†’ Root Cause

```mermaid
flowchart TD

A["ALERT or User Complaint"] --> B{"What is the symptom?"}

B -->|Slow responses / High latency| C["Check Traffic Dashboard"]
B -->|Errors or Crashes| D["Check Application Dashboard"]
B -->|Node or Container down| E["Check Node/Pod Dashboard"]
B -->|Disk Full| F["Check Disk Dashboard"]

%% Latency path
C --> C1{"Is traffic normal?"}
C1 -->|Yes| C2["Check CPU & Memory"]
C1 -->|No - Spike| C3["Check scaling"]

C2 -->|CPU > 90%| C4["Scale out or optimize app"]
C2 -->|Memory/Swap high| C5["Possible leak or undersized - fix or resize"]
C2 -->|Both OK| C6["Check Database latency"]

C3 -->|Scaling works| C4
C3 -->|Scaling blocked| C7["Add caching/CDN, rate limiting"]

C6 -->|DB slow| C8["Optimize queries, add replicas"]
C6 -->|DB fine| C9["Check network"]

C9 -->|Packet loss high| C10["Investigate NIC / load balancer"]
C9 -->|Network fine| C11["Check external dependencies"]

%% Errors path
D --> D1{"Error type?"}
D1 -->|HTTP 500s| D2["Check logs - DB errors/config issues"]
D1 -->|CrashLoop| D3["Check container memory/CPU limits"]
D1 -->|OOMKilled| D4["Raise memory or fix leaks"]

%% Node/Pod path
E --> E1{"Which failed?"}
E1 -->|Node down| E2["Check disk, power, kernel logs"]
E1 -->|Pod down| E3["Check healthchecks, events, logs"]

%% Disk path
F --> F1{"Disk usage > 80%?"}
F1 -->|Yes - Logs| F2["Check log growth, tmp files"]
F1 -->|Yes - Capacity| F3["Rotate/compress logs, expand disk"]
F1 -->|No but I/O high| F4["Check IOPS & latency"]

```

---

## ğŸ§© How to Use This Flow

1. **Start at the Symptom**

   * Alert: high latency, errors, disk full, pod restarts.
   * User complaint: â€œsite is slowâ€ / â€œapp keeps crashingâ€.

2. **Pick the Path**

   * Latency â†’ traffic â†’ CPU/mem â†’ DB â†’ network.
   * Errors â†’ logs â†’ DB/app configs â†’ containers.
   * Container down â†’ check limits â†’ healthchecks â†’ logs.
   * Disk full â†’ check log growth â†’ cleanup â†’ expand.

3. **Correlate Across Layers**

   * Example: high latency + high CPU = bottleneck.
   * Example: high latency + normal CPU/mem = likely DB or network.

---

## ğŸ“Š Key Dashboards to Check Along the Way

| Step          | Dashboard / Metric                       | What It Tells You                |
| ------------- | ---------------------------------------- | -------------------------------- |
| Traffic spike | Prometheus `http_requests_total`         | Is the load abnormal?            |
| CPU usage     | Node exporter `node_cpu_seconds_total`   | Is system CPU bound?             |
| Memory & Swap | Node exporter `node_memory_MemAvailable` | Is system thrashing?             |
| Containers    | cAdvisor / K8s pod metrics               | Is a pod OOMKilled or throttled? |
| DB latency    | postgres\_exporter / mysql\_exporter     | Are queries slow?                |
| Disk usage    | node\_exporter filesystem                | Disk nearly full?                |
| Disk I/O      | node\_disk\_read/write\_time             | Storage bottleneck?              |
| Network       | node\_network\_errors\_total             | NIC drops/retransmits?           |
| Logs          | Loki / ELK / `docker logs`               | The â€œwhyâ€ behind the metrics.    |

---

## ğŸ› ï¸ Example Walkthroughs with Flow

### Example 1: Slow Website

* Latency up â†’ traffic spike.
* CPU at 95% â†’ bottleneck.
* Fix: scale replicas from 3 â†’ 6.

### Example 2: High Error Rate

* Errors are HTTP 500.
* Logs: â€œDB connection failedâ€.
* DB connections maxed.
* Fix: increase pool size, optimize queries.

### Example 3: Container Restarting

* Pod in CrashLoop.
* Container OOMKilled.
* Fix: raise memory limit, tune heap size.

### Example 4: Disk Full

* Disk usage 95%.
* `/var/log` growing fast.
* Fix: rotate logs, expand disk.

---

# ğŸ¯ DevOps Mindset

* Always think in **layers**:
  **User â†’ Load Balancer â†’ App â†’ DB â†’ OS â†’ Hardware/Network**.
* Use dashboards to narrow down *which layer is misbehaving*.
* Confirm with logs to know *why*.
* Fix immediate issue â†’ plan long-term solution.

---

## ğŸ› ï¸ Troubleshooting Examples with Dashboards

## ğŸ”¥ Scenario 1: Website is Slow (High Latency)

### Symptom:

* Users report: *â€œSite feels sluggishâ€*.
* Grafana shows latency rising above 2s.

### Step 1: Check Traffic

* **Dashboard:** Request rate (Prometheus `http_requests_total`).
* **Observation:** Traffic is 2Ã— higher than normal.
* ğŸ‘‰ Spike in demand.

### Step 2: Check CPU

* **Dashboard:** Node exporter CPU usage.
* **Observation:** CPU at 95%.
* ğŸ‘‰ Server is CPU-bound.

### Step 3: Correlate with App

* **Dashboard:** Container resource usage (cAdvisor).
* **Observation:** The web app container is consuming most CPU.

### Likely Cause:

* Increased load â†’ CPU saturated.

### Fix:

* Scale replicas from 3 â†’ 6.
* Add caching (e.g., Cloudflare / Redis).

---

## ğŸ”¥ Scenario 2: High Error Rate (HTTP 500s)

### Symptom:

* Alert: *â€œ5% of requests failing with 500 errorsâ€*.

### Step 1: Check Application Logs

* **Dashboard link:** Loki/ELK.
* **Observation:** â€œDB connection failed: too many connectionsâ€.

### Step 2: Check Database Metrics

* **Dashboard:** PostgreSQL exporter.
* **Observation:** Connection count at max (100).

### Step 3: Check Query Latency

* **Observation:** Queries piling up, high wait time.

### Likely Cause:

* DB connection pool exhausted.

### Fix:

* Increase DB connection pool size.
* Optimize slow queries.
* Add a DB read replica if traffic keeps growing.

---

## ğŸ”¥ Scenario 3: Container Keeps Restarting

### Symptom:

* Alert: *â€œApp container restarting repeatedly (CrashLoopBackOff)â€*.

### Step 1: Check Container Restarts

* **Dashboard:** cAdvisor / Kubernetes pod metrics.
* **Observation:** Container restarted 12 times in 5 minutes.

### Step 2: Check Memory Usage

* **Observation:** Container hits memory limit (512MB), then OOMKilled.

### Step 3: Check Logs

* **Observation:** Java app error: â€œOutOfMemoryError: Java heap spaceâ€.

### Likely Cause:

* App exceeds container memory limits.

### Fix:

* Increase memory limit to 1GB.
* Tune JVM heap settings.
* Monitor for leaks.

---

## ğŸ”¥ Scenario 4: Disk Full

### Symptom:

* Alert: *â€œDisk usage > 90%â€*.

### Step 1: Check Disk Usage

* **Dashboard:** Node exporter filesystem metrics.
* **Observation:** `/var/log` partition at 95%.

### Step 2: Drill Into Log Volume

* **Observation:** App logs growing at 1GB/hour.

### Step 3: Correlate With Traffic

* **Observation:** Error logs spiking with traffic surge.

### Likely Cause:

* App spamming logs due to repeated errors.

### Fix:

* Fix root cause of error.
* Rotate/compress logs.
* Increase disk size if needed.

---

## ğŸ”¥ Scenario 5: Network Latency Spikes

### Symptom:

* Alert: *â€œP95 latency > 500msâ€*.

### Step 1: Check Network Dashboard

* **Observation:** High packet retransmits on one node.

### Step 2: Check Node Health

* **Observation:** NIC errors increasing.

### Step 3: Correlate With Service

* **Observation:** Only services on node-2 affected.

### Likely Cause:

* Faulty NIC or driver issue on node-2.

### Fix:

* Drain node-2 (`kubectl cordon node-2`).
* Replace or troubleshoot hardware.

---

## ğŸ”¥ Scenario 6: Prometheus Missing Metrics

### Symptom:

* Dashboard panel blank.

### Step 1: Check Prometheus Targets

* **Dashboard:** Prometheus `/targets` UI.
* **Observation:** Target `node-exporter:9100` is `DOWN`.

### Step 2: Check Service

* **Observation:** node-exporter container stopped.

### Step 3: Check Logs

* **Observation:** Error binding to port `9100`.

### Likely Cause:

* Port conflict or container crash.

### Fix:

* Restart node-exporter.
* Ensure no other process using 9100.

---

## ğŸ“Š Quick Troubleshooting Playbook

| Symptom            | Where to Look             | Common Causes                                    | Actions                                |
| ------------------ | ------------------------- | ------------------------------------------------ | -------------------------------------- |
| High latency       | CPU, DB latency           | CPU saturation, slow queries, network congestion | Scale out, add cache, optimize queries |
| High error rate    | Logs, app metrics         | DB pool exhausted, app crash, misconfig          | Fix DB, check app health               |
| Container restarts | Container metrics, logs   | OOMKilled, crashloop, bad healthcheck            | Increase limits, debug app             |
| Disk full          | Node exporter disk        | Log growth, data not rotated                     | Cleanup, rotate logs                   |
| Network drops      | NIC stats, node dashboard | Faulty NIC, congestion                           | Replace NIC, move workload             |
| Missing metrics    | Prometheus targets        | Exporter down, scrape failure                    | Restart exporter, fix config           |

---

âœ… With this playbook:

* Start at **symptom** (latency, errors, restarts).
* Drill into **dashboards** (CPU/mem/disk/net/app).
* Correlate across **layers** (infra â†’ container â†’ app â†’ DB).
* Confirm with **logs**.
* Apply **fix or scale**.

---